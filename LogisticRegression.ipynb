{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2LogisticRegression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MicroprocessorX069/Handwriting-comparison/blob/master/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4cCE3IdWNU9G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0BlHJstsDhM8",
        "colab_type": "code",
        "outputId": "bfaefd45-27a5-497f-efaf-54ed6fd9512a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "Dataset2=pd.read_csv('allDataConcat.csv')\n",
        "print(np.array(Dataset2).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1469114, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2PBMSKp4xeYu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Dataset1=np.array(pd.read_csv('allDataSubtract.csv'))\n",
        "Dataset2=np.array(pd.read_csv('allDataConcat.csv'))\n",
        "Dataset3=np.array(pd.read_csv('gscSubtract2.csv'))\n",
        "Dataset4=np.array(pd.read_csv('gscConcat.csv'))\n",
        "\n",
        "#Slicing for easier computation for viewing the results\n",
        "Dataset1=np.array(Dataset1[0:100000,:])\n",
        "Dataset2=np.array(Dataset2[0:100000,:])\n",
        "Dataset3=np.array(Dataset3[0:10000,:])\n",
        "Dataset3=np.array(Dataset3[0:10000,:])\n",
        "#Parameters\n",
        "learningRate=0.01\n",
        "batchSize=10000\n",
        "train_split=0.9\n",
        "validation_split=0\n",
        "no_basisFn=8\n",
        "features_concat=18\n",
        "features_subtract=9\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.array(gradDescentLogistic(Dataset1,feature_no=features_subtract,train_percent=train_split,validation_percent=validation_split,m=no_basisFn , batch_size=batchSize,learning_rate=learningRate)))\n",
        "plt.plot(np.array(gradDescentLogistic(Dataset2,feature_no=features_concat,train_percent=train_split,validation_percent=validation_split,m=no_basisFn , batch_size=batchSize,learning_rate=learningRate)))\n",
        "plt.plot(np.array(gradDescentLogistic(Dataset3,feature_no=features_subtract,train_percent=train_split,validation_percent=validation_split,m=no_basisFn , batch_size=batchSize,learning_rate=learningRate)))\n",
        "plt.plot(np.array(gradDescentLogistic(Dataset4,feature_no=features_concat,train_percent=train_split,validation_percent=validation_split,m=no_basisFn , batch_size=batchSize,learning_rate=learningRate)))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mXTk7GTrUiJL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_dataset(Dataset,no_features,train_percent=0.8,validation_percent=0.1,test_percent=0.1):\n",
        "  train_start=0\n",
        "  train_end=train_start+int(Dataset.shape[0]*train_percent)\n",
        "  train_data=Dataset[train_start:train_end,0:no_features]\n",
        "  train_result=Dataset[train_start:train_end,no_features]\n",
        "  \n",
        "  validation_start=train_end+1\n",
        "  validation_end=validation_start+int(Dataset.shape[0]*validation_percent)\n",
        "  validation_data=Dataset[validation_start:validation_end,0:no_features]\n",
        "  validation_result=Dataset[validation_start:validation_end,no_features]\n",
        "  \n",
        "  test_start=validation_end+1\n",
        "  test_end=test_start+int(Dataset.shape[0]*test_percent)\n",
        "  test_data=Dataset[test_start:test_end,0:no_features]\n",
        "  test_result=Dataset[test_start:test_end,no_features]\n",
        "  \n",
        "  return train_data, train_result, validation_data, validation_result, test_data, test_result\n",
        "\n",
        "def createDataset(df,train_percent, validation_percent):\n",
        "  df=df.loc[:,'Af1':'target']\n",
        "  length=df.shape[0]\n",
        "  \n",
        "  trainEnd=int(length*train_percent)\n",
        "  \n",
        "  validationEnd=trainEnd+int(length*validation_percent)\n",
        "  \n",
        "  trainData=df.iloc[0:trainEnd]\n",
        "  trainResult=trainData.loc[:,'target']\n",
        "  trainData=trainData.loc[:,'Af1':'Bf9']\n",
        "  trainResult=trainResult.reset_index(drop=True)\n",
        "  trainData=trainData.reset_index(drop=True)\n",
        "  \n",
        "  validationData=df.iloc[trainEnd:validationEnd]\n",
        "  validationResult=validationData.loc[:,'target']\n",
        "  validationData=validationData.loc[:,'Af1':'Bf9']\n",
        "  validationResult=validationResult.reset_index(drop=True)\n",
        "  validationData=validationData.reset_index(drop=True)\n",
        "  \n",
        "  testData=df.iloc[validationEnd:length]\n",
        "  testResult=testData.loc[:,'target']\n",
        "  testData=testData.loc[:,'Af1':'Bf9']\n",
        "  testResult=testResult.reset_index(drop=True)\n",
        "  testData=testData.reset_index(drop=True)\n",
        "  \n",
        "  return trainData, trainResult, validationData, validationResult, testData, testResult\n",
        "  \n",
        "def clusterIndices(no_clusters, labels_array): #numpy \n",
        "  for cluster_no in range(no_clusters):\n",
        "    \n",
        "   return np.where(labels_array == no_clusters)[0]\n",
        "\n",
        "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic):\n",
        "  # To calculate BigSigma as\n",
        "  #BigSigma 41 x 41\n",
        "  #Matrix of variances between input features, to calculate PhiMatrix\n",
        "    '''\n",
        "                          x(1)      x(2)      x(3) .  .  .     x(41)\n",
        "    BigSigma =   x(1)  Var(1,1)      0         0                 0\n",
        "\n",
        "                 x(2)       0      Var(2,2)    0                 0\n",
        "\n",
        "                 x(3)       0       0        Var(3,3)            0\n",
        "                  .\n",
        "                  .\n",
        "                  .\n",
        "                 x(41)                                      Var(41,41)\n",
        "      '''        \n",
        "    \n",
        "    #Initializing BigSigma matrix as zeros since we just have to populate the diagonal elements\n",
        "    \n",
        "    BigSigma    = np.zeros((len(Data),len(Data))) #shape = 41,41\n",
        "    DataT       = np.transpose(Data) #len(DataT)=69k \n",
        "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01)) #math ceil return smallest integer less than X, basically converting to nearest bigger int #Len of training data set = 80% of 69k ~ 67k \n",
        "    varVect     = []\n",
        "    \n",
        "    # Calculating the variance for each input feature, based on training data. np.var return variance of an array\n",
        "    for i in range(0,len(DataT[0])):\n",
        "        vct = []\n",
        "        for j in range(0,int(TrainingLen)):\n",
        "            vct.append(Data[i][j])    \n",
        "        varVect.append(np.var(vct)) #Varianance of Data[i,;] i.e. input feature i\n",
        "    \n",
        "    #Populating BigSigma Matrix\n",
        "    for j in range(len(Data)):\n",
        "        BigSigma[j][j] = varVect[j] ## All other indices other than j,j (i.e. where i != j) would be zero since the covariance between two input features is no required.\n",
        "    if IsSynthetic == True:         # Use of isSynthetic not known yet\n",
        "        BigSigma = np.dot(3,BigSigma)\n",
        "    else:\n",
        "        BigSigma = np.dot(200,BigSigma)\n",
        "    ##print (\"BigSigma Generated..\")\n",
        "    return BigSigma\n",
        "\n",
        "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
        "    #Converts the matrices' (x-Mu), BigSigmaInverse, (x-Mu) product to scalar\n",
        "    \n",
        "    #R is (x-Mu); Shape : 1 x 41\n",
        "    R = np.subtract(DataRow,MuRow)\n",
        "    #np.transpose(R) is Transpose of (x-Mu); Shape :  41 x 1\n",
        "    # Shape of BigSigInv is 41 x 41\n",
        "    # Therefore Shape of T i.e. (R x BigSigmaInverse x Inv(R)): (41) x 41) . (41 x 1)\n",
        "    #                                                          : 41 x 1 \n",
        "    T = np.dot(BigSigInv,np.transpose(R))  \n",
        "    L = np.dot(R,T) #(R . T)\n",
        "    # Therefore Shape of T i.e. (R x BigSigmaInverse x Inv(R)): (1 x 41). (41) x 41) . (41 x 1)\n",
        "    #                                                          : 1 x 1 # HenceScalaar\n",
        "    return L\n",
        "\n",
        "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):  \n",
        "  #Calculation of Phi(x)\n",
        "  #Vector Phi(x) = (-0.5 . (x-Mu) . BigSigmaInverse . (x-Mu))\n",
        "   #              e\n",
        "    # math.exp is exponential function e^(x) where x is the argument of the function\n",
        "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
        "    return phi_x\n",
        "\n",
        "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
        "    #Phi Matrix Shape:  No. of basis fns x Dataset_height i.e. 10 x 67k\n",
        "    # 10 Basis function matrix applied on 41 input feautres\n",
        "    '''\n",
        "    #Phi(x(1)(1/10)) Means Basis function of 1/10 th part of input feature 1 (We divided the input features with kmeans below into 10 clusters)\n",
        "    \n",
        "    Vector Phi(x) = (-0.5 . (x-Mu) . BigSigmaInverse . (x-Mu))\n",
        "                   e\n",
        "                   \n",
        "                   \n",
        "                        Phi1                   Phi2                     Phi3                    .  .  .     Phi10    # no of basis functions i.e. 10\n",
        "  PhiMatrix=   x(1)   Phi(x(1)(1/10))      Phi(x(1)(2/10))         Phi(x(1)(3/10))     .  .  .        Phi(x(1)(10/10))\n",
        "  \n",
        "               x(2)   Phi(x(2)(1/10))      Phi(x(2)(2/10))         Phi(x(2)(3/10))     .  .  .        Phi(x(2)(10/10))\n",
        "    \n",
        "               x(3)   Phi(x(3)(1/10))      Phi(x(3)(2/10))         Phi(x(3)(3/10))     .  .  .        Phi(x(3)(10/10))\n",
        "                .\n",
        "                .\n",
        "                .\n",
        "              x(67k)  Phi(x(67k)(1/10))    Phi(x(67k)(2/10))       Phi(x(67k)(3/10))     .  .  .      Phi(x(67k)(1/10))\n",
        "    '''        \n",
        "    DataT = np.transpose(Data)\n",
        "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
        "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
        "    \n",
        "    # Calculating inverse of BigSigma matrix for the formula of phi\n",
        "    BigSigInv = np.linalg.pinv(BigSigma)\n",
        "    for  C in range(0,len(MuMatrix)):\n",
        "        for R in range(0,int(TrainingLen)):\n",
        "            #Calculating Phi of each x1\n",
        "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
        "    #print (\"PHI Generated..\")\n",
        "    return PHI\n",
        "  \n",
        "import math\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "  \n",
        "def getOutput(input_data, weights):\n",
        "  predicted_result=[]\n",
        "  for input in input_data:\n",
        "      output=0\n",
        "      for i in range(1,len(weights)):\n",
        "        output=output+weights[i]*input[i-1]\n",
        "      output=sigmoid(output)\n",
        "      predicted_result.append(output)\n",
        "  return(np.array(predicted_result))\n",
        "\n",
        "def updateWeights(input_data,target_result,predicted_result,prev_weights,learning_rate):\n",
        "  subtract=np.subtract(predicted_result,target_result)\n",
        "  weight_change=np.dot(subtract,input_data)\n",
        "  weight_change/=(input_data.shape[1])\n",
        "  weight_change*=learning_rate\n",
        "  new_weights=np.subtract(prev_weights,weight_change)\n",
        "  return new_weights\n",
        "\n",
        "def loss(target_result,predicted_result):\n",
        "  loss=-target_result*np.log(predicted_result)-(1-target_result)*np.log(1-predicted_result)\n",
        "  loss=loss.sum()/target_result.shape[0]\n",
        "  return loss\n",
        "  \n",
        "def grad_descent(train_phi,trainResult,validation_phi,validationResult,batch_size,learning_rate=0.1):\n",
        "  W=np.zeros(train_phi.shape[1])\n",
        "  i=0\n",
        "  loss_arr=[]\n",
        "  while i+batch_size <= train_phi.shape[0]:\n",
        "    input_data=train_phi[i:i+batch_size]\n",
        "    target_result=trainResult[i:i+batch_size]\n",
        "    i=i+batch_size\n",
        "    new_lr=learning_rate-(1/i)\n",
        "    if(new_lr<=0):\n",
        "      new_lr=learning_rate\n",
        "    predicted_result=getOutput(input_data,W)\n",
        "    validation_predictedResult=getOutput(validation_phi,W)\n",
        "    W=updateWeights(input_data,target_result,predicted_result,W,learning_rate)\n",
        "    loss_arr.append(loss(validationResult,validation_predictedResult))\n",
        "  return(loss_arr)   \n",
        "\n",
        "def gradDescentLogistic(Dataset,feature_no, train_percent=0.9,validation_percent=0,m=8,IsSynthetic=False,batch_size=20000,learning_rate=0.01):\n",
        "  trainData, trainResult, validationData, validationResult, testData, testResult = create_dataset(Dataset,feature_no,train_percent, validation_percent)\n",
        "  trainData=np.array(trainData)\n",
        "  trainResult=np.array(trainResult)\n",
        "  testData=np.array(testData)\n",
        "  testResult=np.array(testResult)\n",
        "\n",
        "  from sklearn.cluster import KMeans\n",
        "\n",
        "  trainTranspose=np.transpose(trainData)\n",
        "  kmeans=KMeans(m,random_state=0).fit(trainData)\n",
        "  means=np.array(kmeans.cluster_centers_)\n",
        "  variance_mat     = GenerateBigSigma(trainTranspose, means,100 ,IsSynthetic) # generating variance for each input feature\n",
        "  train_phi = GetPhiMatrix(trainTranspose, means, variance_mat, 100)\n",
        "  print(\"Training Data shape: \",train_phi.shape)\n",
        "\n",
        "  '''validationTranspose=np.transpose(validationData)\n",
        "  kmeans=KMeans(m,random_state=0).fit(validationData)\n",
        "  means=np.array(kmeans.cluster_centers_)\n",
        "  #ClusterIndicesNumpy(7, kmeans.labels_)\n",
        "  variance_mat     = GenerateBigSigma(validationTranspose, means,100 ,IsSynthetic) # generating variance for each input feature\n",
        "  validation_phi = GetPhiMatrix(validationTranspose, means, variance_mat, 100)\n",
        "  print(\"Validation Data shape: \",train_phi.shape)\n",
        "  '''\n",
        "  testTranspose=np.transpose(testData)\n",
        "  kmeans=KMeans(m,random_state=0).fit(testData)\n",
        "  means=np.array(kmeans.cluster_centers_)\n",
        "  #ClusterIndicesNumpy(7, kmeans.labels_)\n",
        "  variance_mat     = GenerateBigSigma(testTranspose, means,100 ,IsSynthetic) # generating variance for each input feature\n",
        "  test_phi = GetPhiMatrix(testTranspose, means, variance_mat, 100)\n",
        "  print(\"Test Data shape: \",test_phi.shape)\n",
        "  loss_arr=grad_descent(train_phi,trainResult,test_phi, testResult,batch_size)\n",
        "  return loss_arr\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}